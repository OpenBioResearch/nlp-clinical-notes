{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NLP Pipeline using Clinician Notes\n",
        "\n",
        "This Jupyter notebook provides an NLP pipeline for exploring clinician notes, leveraging Natural Language Processing (NLP) techniques to analyze and cluster clinical notes. It utilizes BioBERT, which is pre-trained on large-scale biomedical corpora and can generate context-aware embeddings for clinical notes.While the primary function of this script is to explore NLP tools for clinician-patient notes, adapting this script to help identify relationships to sepsis could provide insights for clinical decision-making."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "import spacy\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "directory = os.getenv('CLINICAL_REFS_DIR', \"./clinical_refs\")\n",
        "\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "\n",
        "train_file_path = os.path.join(directory, 'MTS-Dialog-TrainingSet.csv')\n",
        "validation_file_path = os.path.join(directory, 'MTS-Dialog-ValidationSet.csv')\n",
        "\n",
        "train_df = pd.read_csv(train_file_path)\n",
        "validation_df = pd.read_csv(validation_file_path)\n",
        "\n",
        "# retrieve headers\n",
        "train_headers = train_df.columns.tolist()\n",
        "validation_headers = validation_df.columns.tolist()\n",
        "\n",
        "print(\"Training dataset columns:\", train_headers)\n",
        "print(\"Validation dataset columns:\", validation_headers)\n",
        "print(\"Training dataset preview:\")\n",
        "print(train_df.head())\n",
        "print(\"Validation dataset preview:\")\n",
        "print(validation_df.head())\n",
        "\n",
        "# spacy model with error handling\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    print(\"SpaCy model loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading SpaCy model: {e}\")\n",
        "    print(\"Attempting to download the model...\")\n",
        "    try:\n",
        "        import spacy.cli\n",
        "        spacy.cli.download(\"en_core_web_sm\")\n",
        "        nlp = spacy.load(\"en_core_web_sm\")\n",
        "        print(\"SpaCy model downloaded and loaded successfully.\")\n",
        "    except Exception as download_error:\n",
        "        print(f\"Error downloading or loading SpaCy model: {download_error}\")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6Kgn9ieSAt7"
      },
      "source": [
        "###  Preprocessing \n",
        "Removing irrelevant data, Tokenizing  text, removing stop words,lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "default_stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Add domain-specific stop words (extend this list as needed)\n",
        "medical_stop_words = {'patient', 'doctor', 'mg', 'ml'}\n",
        "stop_words = default_stop_words.union(medical_stop_words)\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocess the input text by:\n",
        "    1. Lowercasing the text\n",
        "    2. Removing non-alphanumeric characters\n",
        "    3. Tokenizing the text\n",
        "    4. Removing stop words\n",
        "    5. Lemmatizing the words\n",
        "    \"\"\"\n",
        "    try:\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'\\W', ' ', text)\n",
        "\n",
        "        # Tokenize the text using Spacy\n",
        "        doc = nlp(text)\n",
        "        tokens = [token.lemma_ for token in doc if token.text not in stop_words and not token.is_punct and not token.is_space]\n",
        "        processed_text = ' '.join(tokens)\n",
        "\n",
        "        return processed_text\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error during text preprocessing: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "train_df['processed_text'] = train_df['section_text'].apply(preprocess_text)\n",
        "validation_df['processed_text'] = validation_df['section_text'].apply(preprocess_text)\n",
        "\n",
        "print(\"Processed Training DataFrame:\")\n",
        "print(train_df[['ID', 'section_text', 'processed_text']].head())\n",
        "\n",
        "print(\"Processed Validation DataFrame:\")\n",
        "print(validation_df[['ID', 'section_text', 'processed_text']].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_ZqTc6ZWyn_"
      },
      "source": [
        "### Feature Extraction using BioBert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Initialize BioBERT model and tokenizer from Hugging Face\n",
        "model_name = \"dmis-lab/biobert-base-cased-v1.1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "def embed_text(text, tokenizer, model):\n",
        "    try:\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        # Use the embeddings of the [CLS] token\n",
        "        cls_embedding = outputs.last_hidden_state[:, 0, :].numpy().flatten()\n",
        "        return cls_embedding\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error during text embedding: {e}\")\n",
        "        return np.zeros(model.config.hidden_size)\n",
        "\n",
        "train_df['embedding'] = train_df['processed_text'].apply(lambda x: embed_text(x, tokenizer, model))\n",
        "validation_df['embedding'] = validation_df['processed_text'].apply(lambda x: embed_text(x, tokenizer, model))\n",
        "\n",
        "# Convert embeddings to a numpy array for clustering\n",
        "X_train = np.vstack(train_df['embedding'].values)\n",
        "X_validation = np.vstack(validation_df['embedding'].values)\n",
        "\n",
        "print(f\"Shape of X_train: {X_train.shape}\")\n",
        "print(f\"Shape of X_validation: {X_validation.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csJEiWuBZ7dk"
      },
      "source": [
        "### Model Selection and Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def apply_kmeans_clustering(X_train, X_validation, num_clusters=2):\n",
        "    \"\"\"\n",
        "    Apply K-Means clustering to the training and validation sets.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "        kmeans.fit(X_train)\n",
        "\n",
        "        train_clusters = kmeans.predict(X_train)\n",
        "        validation_clusters = kmeans.predict(X_validation)\n",
        "\n",
        "        logging.info(\"K-Means clustering completed successfully.\")\n",
        "        return train_clusters, validation_clusters, kmeans\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error during K-Means clustering: {e}\")\n",
        "        raise\n",
        "\n",
        "try:\n",
        "    num_clusters = 3  # Adjust the number of clusters as needed\n",
        "    train_clusters, validation_clusters, kmeans = apply_kmeans_clustering(X_train, X_validation, num_clusters)\n",
        "\n",
        "    train_df['cluster'] = train_clusters\n",
        "    validation_df['cluster'] = validation_clusters\n",
        "\n",
        "    print(\"Training DataFrame with cluster labels:\")\n",
        "    print(train_df[['ID', 'section_text', 'cluster']].head())\n",
        "    print(\"Validation DataFrame with cluster labels:\")\n",
        "    print(validation_df[['ID', 'section_text', 'cluster']].head())\n",
        "\n",
        "except Exception as e:\n",
        "    logging.error(f\"Error during K-Means clustering setup: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def analyze_clusters(df, num_clusters):\n",
        "    cluster_analysis = {}\n",
        "    sepsis_keywords = ['sepsis', 'septic', 'infection', 'organ failure', 'blood culture', 'antibiotics']\n",
        "\n",
        "    for cluster_id in range(num_clusters):\n",
        "        cluster_texts = df[df['cluster'] == cluster_id]['processed_text'].tolist()\n",
        "        word_counter = Counter(' '.join(cluster_texts).split())\n",
        "\n",
        "        sepsis_keyword_count = {keyword: word_counter[keyword] for keyword in sepsis_keywords}\n",
        "        total_words = sum(word_counter.values())\n",
        "\n",
        "        relative_frequency = {keyword: count / total_words for keyword, count in sepsis_keyword_count.items()}\n",
        "\n",
        "        cluster_analysis[cluster_id] = {\n",
        "            'total_texts': len(cluster_texts),\n",
        "            'sepsis_keyword_count': sepsis_keyword_count,\n",
        "            'relative_frequency': relative_frequency\n",
        "        }\n",
        "\n",
        "    return cluster_analysis\n",
        "\n",
        "num_clusters = 3\n",
        "cluster_analysis = analyze_clusters(train_df, num_clusters)\n",
        "\n",
        "for cluster_id, analysis in cluster_analysis.items():\n",
        "    print(f\"\\nCluster {cluster_id} Analysis:\")\n",
        "    print(f\"Total texts: {analysis['total_texts']}\")\n",
        "    print(\"Sepsis Keyword Count:\", analysis['sepsis_keyword_count'])\n",
        "    print(\"Relative Frequency of Sepsis Keywords:\", analysis['relative_frequency'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Evaluation using Silhouette Score and Davies-Bouldin Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
        "\n",
        "def evaluate_clustering(X, clusters):\n",
        "    try:\n",
        "        silhouette_avg = silhouette_score(X, clusters)\n",
        "        print(f\"Silhouette Score: {silhouette_avg:.4f}\")\n",
        "\n",
        "        davies_bouldin_avg = davies_bouldin_score(X, clusters)\n",
        "        print(f\"Davies-Bouldin Index: {davies_bouldin_avg:.4f}\")\n",
        "\n",
        "        logging.info(\"Clustering evaluation metrics calculated successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error during clustering evaluation: {e}\")\n",
        "        raise\n",
        "\n",
        "evaluate_clustering(X_train, train_df['cluster'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igASe-dubXke"
      },
      "source": [
        "### Model Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Define sepsis-related keywords grouped by categories\n",
        "sepsis_keywords = {\n",
        "    'symptoms': ['fever', 'tachycardia', 'hypotension', 'leukocytosis', 'respiratory distress'],\n",
        "    'diagnosis': ['sepsis', 'septic', 'bacteremia', 'blood culture', 'inflammatory response', 'systemic'],\n",
        "    'treatment': ['antibiotics', 'shock', 'organ failure', 'multi-organ failure']\n",
        "}\n",
        "\n",
        "def analyze_cluster_themes(df, num_clusters, sepsis_keywords):\n",
        "    \"\"\"\n",
        "    Analyze the themes of each cluster based on the frequency of sepsis-related keywords.\n",
        "    \"\"\"\n",
        "    cluster_themes = {}\n",
        "\n",
        "    for cluster_id in range(num_clusters):\n",
        "        cluster_texts = df[df['cluster'] == cluster_id]['processed_text'].tolist()\n",
        "        word_counter = Counter(' '.join(cluster_texts).split())\n",
        "\n",
        "        theme_counts = {theme: sum(word_counter[keyword] for keyword in keywords) for theme, keywords in sepsis_keywords.items()}\n",
        "        cluster_themes[cluster_id] = theme_counts\n",
        "\n",
        "    return cluster_themes\n",
        "\n",
        "def plot_clusters_tsne_with_themes(X, clusters, num_clusters, cluster_themes):\n",
        "    try:\n",
        "        # Apply t-SNE for dimensionality reduction\n",
        "        tsne = TSNE(n_components=2, perplexity=40, random_state=42)\n",
        "        X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "        # Create a DataFrame for t-SNE results\n",
        "        tsne_df = pd.DataFrame(X_tsne, columns=['tsne1', 'tsne2'])\n",
        "        tsne_df['cluster'] = clusters\n",
        "\n",
        "        dominant_themes = {cluster_id: max(themes, key=themes.get) for cluster_id, themes in cluster_themes.items()}\n",
        "\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        for cluster_id in range(num_clusters):\n",
        "            cluster_data = tsne_df[tsne_df['cluster'] == cluster_id]\n",
        "            plt.scatter(cluster_data['tsne1'], cluster_data['tsne2'], label=f'Cluster {cluster_id} ({dominant_themes[cluster_id]})', alpha=0.6)\n",
        "        \n",
        "        plt.xlabel(\"t-SNE Dimension 1\")\n",
        "        plt.ylabel(\"t-SNE Dimension 2\")\n",
        "        plt.title(\"t-SNE Visualization of Clusters with Themes\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "        logging.info(\"t-SNE cluster plot with themes generated successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error during t-SNE plotting with themes: {e}\")\n",
        "        raise\n",
        "\n",
        "num_clusters = 3\n",
        "cluster_themes = analyze_cluster_themes(train_df, num_clusters, sepsis_keywords)\n",
        "\n",
        "for cluster_id, themes in cluster_themes.items():\n",
        "    print(f\"\\nCluster {cluster_id} Theme Analysis:\")\n",
        "    print(themes)\n",
        "\n",
        "plot_clusters_tsne_with_themes(X_train, train_df['cluster'], num_clusters, cluster_themes)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNhaLKzhhibdpvDWSwRjL1i",
      "gpuType": "L4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
