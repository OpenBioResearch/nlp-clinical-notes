{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## NLP Pipeline for Clinical Notes Analysis using ClinicalBERT\n",
        "\n",
        "NLP pipeline for processing clinical notes using ClinicalBERT, Stanza biomedical models, and spaCy.  Future Work: Incorporate a different clinical notes dataset to further explore sepsis-related topics, leveraging the existing NLP pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install stanza transformers torch\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import stanza\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from transformers import pipeline\n",
        "\n",
        "# Download Stanza Biomedical models\n",
        "stanza.download('en', package='mimic', processors={'ner': 'bc5cdr'})\n",
        "\n",
        "# Check GPU availability\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU is available and will be used for processing.\")\n",
        "else:\n",
        "    print(\"GPU is not available. The CPU will be used for processing.\")\n",
        "\n",
        "# Load Stanza pipeline for English Biomedical models\n",
        "stanza_nlp = stanza.Pipeline('en', package='mimic', processors={'ner': 'bc5cdr'})\n",
        "\n",
        "# Load ClinicalBERT model and tokenizer for token classification (NER)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
        "\n",
        "# Create a NER pipeline using ClinicalBERT\n",
        "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "directory = os.getenv('CLINICAL_REFS_DIR', \"./clinical_refs\")\n",
        "\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "\n",
        "# File paths\n",
        "train_file_path = os.path.join(directory, 'MTS-Dialog-TrainingSet.csv')\n",
        "validation_file_path = os.path.join(directory, 'MTS-Dialog-ValidationSet.csv')\n",
        "\n",
        "# Load the datasets\n",
        "train_df = pd.read_csv(train_file_path)\n",
        "validation_df = pd.read_csv(validation_file_path)\n",
        "\n",
        "# Explicitly retrieve headers\n",
        "train_headers = train_df.columns.tolist()\n",
        "validation_headers = validation_df.columns.tolist()\n",
        "\n",
        "# Print the column headers to confirm the available columns\n",
        "print(\"Training dataset columns:\", train_headers)\n",
        "print(\"Validation dataset columns:\", validation_headers)\n",
        "\n",
        "# Display the first few rows of each dataset\n",
        "print(\"Training dataset preview:\")\n",
        "print(train_df.head())\n",
        "\n",
        "print(\"Validation dataset preview:\")\n",
        "print(validation_df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocess clinician notes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove non-alphabetic characters and convert to lowercase\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I | re.A).lower()\n",
        "    # Tokenize the text\n",
        "    tokens = text.split()\n",
        "    # Remove stopwords\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# Ensure the 'section_text' column exists in your datasets\n",
        "assert 'section_text' in train_df.columns, \"The training dataset does not have a 'section_text' column.\"\n",
        "assert 'section_text' in validation_df.columns, \"The validation dataset does not have a 'section_text' column.\"\n",
        "\n",
        "train_df['section_text_clean'] = train_df['section_text'].apply(preprocess_text)\n",
        "validation_df['section_text_clean'] = validation_df['section_text'].apply(preprocess_text)\n",
        "\n",
        "\n",
        "print(\"Training dataset preview after preprocessing:\")\n",
        "print(train_df[['section_text', 'section_text_clean']].head())\n",
        "print(\"Validation dataset preview after preprocessing:\")\n",
        "print(validation_df[['section_text', 'section_text_clean']].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  Named Entity Recognition (NER) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure the model is on the correct device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Function to perform NER using ClinicalBERT with text splitting\n",
        "def perform_ner_clinicalbert(text, max_length=512):\n",
        "    tokens = tokenizer(text, return_tensors='pt', truncation=True, max_length=max_length, padding=True)\n",
        "    input_ids = tokens['input_ids']\n",
        "    attention_mask = tokens['attention_mask']\n",
        "    \n",
        "    # Check if input_ids and attention_mask are on the same device as the model\n",
        "    if input_ids.device != device:\n",
        "        input_ids = input_ids.to(device)\n",
        "    if attention_mask.device != device:\n",
        "        attention_mask = attention_mask.to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    logits = outputs.logits\n",
        "\n",
        "    predictions = torch.argmax(logits, dim=2)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].cpu().numpy())\n",
        "    entities = []\n",
        "    for token, prediction in zip(tokens, predictions[0].cpu().numpy()):\n",
        "        if prediction != 0:  # 0 corresponds to the 'O' label in BIO tagging\n",
        "            entities.append((token, model.config.id2label[prediction.item()]))\n",
        "    \n",
        "    return entities\n",
        "\n",
        "# Function to perform NER using Stanza\n",
        "def perform_stanza_ner(text):\n",
        "    doc = stanza_nlp(text)\n",
        "    entities = [(ent.text, ent.type) for ent in doc.ents]\n",
        "    return entities\n",
        "\n",
        "# Apply NER to the training dataset using ClinicalBERT\n",
        "print(\"Applying ClinicalBERT NER to training dataset...\")\n",
        "train_df['clinical_bert_entities'] = train_df['section_text_clean'].apply(perform_ner_clinicalbert)\n",
        "\n",
        "# Apply NER to the validation dataset using ClinicalBERT\n",
        "print(\"Applying ClinicalBERT NER to validation dataset...\")\n",
        "validation_df['clinical_bert_entities'] = validation_df['section_text_clean'].apply(perform_ner_clinicalbert)\n",
        "\n",
        "# Apply NER to the training dataset using Stanza\n",
        "print(\"Applying Stanza NER to training dataset...\")\n",
        "train_df['stanza_entities'] = train_df['section_text_clean'].apply(perform_stanza_ner)\n",
        "\n",
        "# Apply NER to the validation dataset using Stanza\n",
        "print(\"Applying Stanza NER to validation dataset...\")\n",
        "validation_df['stanza_entities'] = validation_df['section_text_clean'].apply(perform_stanza_ner)\n",
        "\n",
        "# Display the results\n",
        "print(\"Training dataset with NER results (ClinicalBERT and Stanza):\")\n",
        "print(train_df[['section_text_clean', 'clinical_bert_entities', 'stanza_entities']].head())\n",
        "\n",
        "print(\"Validation dataset with NER results (ClinicalBERT and Stanza):\")\n",
        "print(validation_df[['section_text_clean', 'clinical_bert_entities', 'stanza_entities']].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sentiment Analysis\n",
        "\n",
        "Note: Sentiment analysis was considered but is not included in the final analysis.\n",
        "Clinical notes tend to have an overwhelmingly negative sentiment due to the nature of the content.\n",
        "Therefore, sentiment analysis does not provide meaningful insights in this context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Relation Extraction using spaCy, Stanza and ClinicalBERT.\n",
        "\n",
        "Bio_ClinicalBERT, as the name suggests, is a variant that combines the strengths of both BioBERT and ClinicalBERT. It is designed to handle a broader range of biomedical and clinical texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "\n",
        "# Load ClinicalBERT model and tokenizer\n",
        "model_name = 'emilyalsentzer/Bio_ClinicalBERT'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device)\n",
        "\n",
        "def truncate_text(text, tokenizer, max_length=512):\n",
        "    # Use tokenizer to encode the text and ensure truncation\n",
        "    inputs = tokenizer(text, max_length=max_length, truncation=True, return_tensors='pt')\n",
        "    # Decode back to string to verify truncation\n",
        "    truncated_text = tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)\n",
        "    return truncated_text\n",
        "\n",
        "def perform_relation_extraction_clinicalbert(text, max_length=512):\n",
        "    # Truncate text to the maximum length\n",
        "    truncated_text = truncate_text(text, tokenizer, max_length)\n",
        "    \n",
        "    # Tokenize and encode the text\n",
        "    tokens = tokenizer(truncated_text, return_tensors='pt', truncation=True, padding=True)\n",
        "    \n",
        "    # Move input tensors to the correct device (GPU or CPU)\n",
        "    input_ids = tokens['input_ids'].to(device)\n",
        "    attention_mask = tokens['attention_mask'].to(device)\n",
        "    \n",
        "    # Perform relation extraction (assuming the model outputs relations)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    \n",
        "    logits = outputs.logits\n",
        "\n",
        "    # Process logits to extract entities and relationships\n",
        "    predictions = torch.argmax(logits, dim=2)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "    entities = []\n",
        "    for token, prediction in zip(tokens, predictions[0]):\n",
        "        if prediction != 0:  # 0 corresponds to the 'O' label in BIO tagging\n",
        "            entities.append((token, model.config.id2label[prediction.item()]))\n",
        "    \n",
        "    # Dummy relationships extraction logic (replace with actual processing logic)\n",
        "    relationships = [(entities[i][0], 'related_to', entities[i+1][0]) for i in range(len(entities)-1)]\n",
        "    \n",
        "    return relationships\n",
        "\n",
        "# Apply relation extraction to the training dataset using ClinicalBERT\n",
        "print(\"Applying ClinicalBERT relation extraction to training dataset...\")\n",
        "train_df['clinicalbert_relations'] = train_df['section_text_clean'].apply(perform_relation_extraction_clinicalbert)\n",
        "\n",
        "# Apply relation extraction to the validation dataset using ClinicalBERT\n",
        "print(\"Applying ClinicalBERT relation extraction to validation dataset...\")\n",
        "validation_df['clinicalbert_relations'] = validation_df['section_text_clean'].apply(perform_relation_extraction_clinicalbert)\n",
        "\n",
        "# Display the first few rows of each dataset after relation extraction\n",
        "print(\"Training dataset after ClinicalBERT relation extraction:\")\n",
        "print(train_df[['section_text_clean', 'clinicalbert_relations']].head())\n",
        "\n",
        "print(\"Validation dataset after ClinicalBERT relation extraction:\")\n",
        "print(validation_df[['section_text_clean', 'clinicalbert_relations']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming Stanza is already set up and stanza_nlp is loaded\n",
        "def extract_stanza_relations(text):\n",
        "    doc = stanza_nlp(text)\n",
        "    relations = []\n",
        "    for sentence in doc.sentences:\n",
        "        for word in sentence.words:\n",
        "            if word.head != 0:  # If the word has a head (root has head 0)\n",
        "                head_word = sentence.words[word.head - 1]\n",
        "                relations.append((word.text, word.deprel, head_word.text))\n",
        "    return relations\n",
        "\n",
        "# Apply relation extraction to the training dataset using Stanza\n",
        "train_df['stanza_relations'] = train_df['section_text_clean'].apply(extract_stanza_relations)\n",
        "print(\"Relation extraction performed on the training dataset using Stanza.\")\n",
        "\n",
        "# Apply relation extraction to the validation dataset using Stanza\n",
        "validation_df['stanza_relations'] = validation_df['section_text_clean'].apply(extract_stanza_relations)\n",
        "print(\"Relation extraction performed on the validation dataset using Stanza.\")\n",
        "\n",
        "# Display the first few rows of each dataset after relation extraction\n",
        "print(\"Training dataset after relation extraction (Stanza):\")\n",
        "print(train_df[['section_text_clean', 'stanza_relations']].head())\n",
        "\n",
        "print(\"Validation dataset after relation extraction (Stanza):\")\n",
        "print(validation_df[['section_text_clean', 'stanza_relations']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "# Load the spaCy model\n",
        "try:\n",
        "    nlp_spacy = spacy.load('en_core_web_sm')\n",
        "except OSError:\n",
        "    from spacy.cli import download\n",
        "    download('en_core_web_sm')\n",
        "    nlp_spacy = spacy.load('en_core_web_sm')\n",
        "\n",
        "def extract_spacy_relations(doc):\n",
        "    # Define the pattern for relation extraction\n",
        "    patterns = [\n",
        "        [{'DEP': 'nsubj'}, {'DEP': 'ROOT'}, {'DEP': 'dobj'}],  # Subject-Verb-Object\n",
        "        [{'DEP': 'nsubj'}, {'DEP': 'prep'}, {'DEP': 'pobj'}],  # Subject-Preposition-Object\n",
        "        [{'DEP': 'nsubjpass'}, {'DEP': 'aux'}, {'DEP': 'prep'}, {'DEP': 'pobj'}]  # Passive Subject-Preposition-Object\n",
        "    ]\n",
        "    \n",
        "    # Initialize the matcher with the patterns\n",
        "    matcher = Matcher(nlp_spacy.vocab)\n",
        "    for i, pattern in enumerate(patterns):\n",
        "        matcher.add(f'relation_pattern_{i}', [pattern])\n",
        "    \n",
        "    matches = matcher(doc)\n",
        "    relations = []\n",
        "    \n",
        "    for match_id, start, end in matches:\n",
        "        span = doc[start:end]\n",
        "        relations.append((span.text, span.root.head.text))\n",
        "    \n",
        "    return relations\n",
        "\n",
        "# Apply relation extraction to the training dataset\n",
        "train_df['spacy_relations'] = train_df['section_text_clean'].apply(lambda x: extract_spacy_relations(nlp_spacy(x)))\n",
        "print(\"Enhanced relation extraction performed on the training dataset using spaCy.\")\n",
        "\n",
        "# Apply relation extraction to the validation dataset\n",
        "validation_df['spacy_relations'] = validation_df['section_text_clean'].apply(lambda x: extract_spacy_relations(nlp_spacy(x)))\n",
        "print(\"Enhanced relation extraction performed on the validation dataset using spaCy.\")\n",
        "\n",
        "print(\"Training dataset after enhanced relation extraction (spaCy):\")\n",
        "print(train_df[['section_text_clean', 'spacy_relations']].head())\n",
        "print(\"Validation dataset after enhanced relation extraction (spaCy):\")\n",
        "print(validation_df[['section_text_clean', 'spacy_relations']].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Topic Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "# Define the number of topics\n",
        "num_topics = 5\n",
        "\n",
        "def perform_topic_modeling(data):\n",
        "    # Vectorize the text data\n",
        "    vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
        "    X = vectorizer.fit_transform(data['section_text_clean'])\n",
        "\n",
        "    # Apply LDA for topic modeling\n",
        "    lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
        "    lda.fit(X)\n",
        "\n",
        "    def display_topics(model, feature_names, num_top_words):\n",
        "        for topic_idx, topic in enumerate(model.components_):\n",
        "            print(f\"Topic {topic_idx}:\")\n",
        "            print(\" \".join([feature_names[i] for i in topic.argsort()[:-num_top_words - 1:-1]]))\n",
        "\n",
        "    num_top_words = 10\n",
        "    display_topics(lda, vectorizer.get_feature_names_out(), num_top_words)\n",
        "\n",
        "print(\"Training dataset topics:\")\n",
        "perform_topic_modeling(train_df)\n",
        "\n",
        "print(\"Validation dataset topics:\")\n",
        "perform_topic_modeling(validation_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install gensim\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.models import LdaModel\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def prepare_text(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "def perform_topic_modeling_gensim(df):\n",
        "    # Tokenize the text\n",
        "    df['tokens'] = df['section_text_clean'].apply(prepare_text)\n",
        "\n",
        "    # Create Dictionary and Corpus needed for Topic Modeling\n",
        "    id2word = corpora.Dictionary(df['tokens'])\n",
        "    texts = df['tokens']\n",
        "    corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "    # Build the LDA model\n",
        "    lda_model = LdaModel(\n",
        "        corpus=corpus, \n",
        "        id2word=id2word, \n",
        "        num_topics=5, \n",
        "        random_state=42, \n",
        "        update_every=1, \n",
        "        chunksize=100, \n",
        "        passes=10, \n",
        "        alpha='auto', \n",
        "        per_word_topics=True\n",
        "    )\n",
        "\n",
        "    # Extract the topics\n",
        "    topics = lda_model.print_topics(num_words=5)\n",
        "\n",
        "    for topic in topics:\n",
        "        print(topic)\n",
        "\n",
        "\n",
        "print(\"Training dataset topics:\")\n",
        "perform_topic_modeling_gensim(train_df)\n",
        "print(\"Validation dataset topics:\")\n",
        "perform_topic_modeling_gensim(validation_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# sepsis\n",
        "\n",
        "!pip install gensim\n",
        "\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import LdaModel\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Step 1: Filter Documents for Sepsis-Related Content\n",
        "def filter_sepsis_documents(df):\n",
        "    sepsis_keywords = ['sepsis', 'septic', 'infection', 'bacteria', 'septicemia']\n",
        "    sepsis_pattern = re.compile(r'\\b(?:' + '|'.join(sepsis_keywords) + r')\\b', re.IGNORECASE)\n",
        "    return df[df['section_text_clean'].apply(lambda text: bool(sepsis_pattern.search(text)))]\n",
        "\n",
        "def prepare_text(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "def perform_topic_modeling_gensim(df, num_topics=5, num_top_words=10):\n",
        "    # Tokenize the text\n",
        "    df['tokens'] = df['section_text_clean'].apply(prepare_text)\n",
        "\n",
        "    # Create Dictionary and Corpus needed for Topic Modeling\n",
        "    id2word = Dictionary(df['tokens'])\n",
        "    texts = df['tokens']\n",
        "    corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "    # Build the LDA model\n",
        "    lda_model = LdaModel(\n",
        "        corpus=corpus, \n",
        "        id2word=id2word, \n",
        "        num_topics=num_topics, \n",
        "        random_state=42, \n",
        "        update_every=1, \n",
        "        chunksize=100, \n",
        "        passes=10, \n",
        "        alpha='auto', \n",
        "        per_word_topics=True\n",
        "    )\n",
        "\n",
        "    topics = lda_model.print_topics(num_words=num_top_words)\n",
        "\n",
        "    for topic in topics:\n",
        "        print(topic)\n",
        "\n",
        "print(\"Training dataset sepsis-related topics:\")\n",
        "filtered_train_df = filter_sepsis_documents(train_df)\n",
        "perform_topic_modeling_gensim(filtered_train_df)\n",
        "\n",
        "print(\"Validation dataset sepsis-related topics:\")\n",
        "filtered_validation_df = filter_sepsis_documents(validation_df)\n",
        "perform_topic_modeling_gensim(filtered_validation_df)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOGKR8On3amuABwi0Pq5vua",
      "gpuType": "L4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
